{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1EmJ_weaELtOXi_GmIZdzYGMCmhiJYQh3","authorship_tag":"ABX9TyMbpsK9gsMa2CKZrEp+n191"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n"],"metadata":{"id":"2p8Msxsg5AFL","executionInfo":{"status":"ok","timestamp":1702312037481,"user_tz":300,"elapsed":512,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["path='/content/drive/MyDrive/Twitter_Data.csv'"],"metadata":{"id":"59ARAgVD5I_o","executionInfo":{"status":"ok","timestamp":1702312037481,"user_tz":300,"elapsed":5,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["twitter_df=pd.read_csv(path)"],"metadata":{"id":"dvk9to-H5Pzo","executionInfo":{"status":"ok","timestamp":1702312038052,"user_tz":300,"elapsed":575,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["path1='/content/drive/MyDrive/Reddit_Data.csv'"],"metadata":{"id":"5GFHg0xI5d7v","executionInfo":{"status":"ok","timestamp":1702312038052,"user_tz":300,"elapsed":7,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["reddit_df=pd.read_csv(path1)"],"metadata":{"id":"FgwsceYp5wlB","executionInfo":{"status":"ok","timestamp":1702312038329,"user_tz":300,"elapsed":282,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Display basic information about the Twitter dataset\n","print(twitter_df.info())\n","\n","# Display basic information about the Reddit dataset\n","print(reddit_df.info())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GfVqMK_3512l","executionInfo":{"status":"ok","timestamp":1702312038501,"user_tz":300,"elapsed":8,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}},"outputId":"46027b11-58d0-4c4e-f238-74e39167ad26"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 162980 entries, 0 to 162979\n","Data columns (total 2 columns):\n"," #   Column      Non-Null Count   Dtype  \n","---  ------      --------------   -----  \n"," 0   clean_text  162976 non-null  object \n"," 1   category    162973 non-null  float64\n","dtypes: float64(1), object(1)\n","memory usage: 2.5+ MB\n","None\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 37249 entries, 0 to 37248\n","Data columns (total 2 columns):\n"," #   Column         Non-Null Count  Dtype \n","---  ------         --------------  ----- \n"," 0   clean_comment  37149 non-null  object\n"," 1   category       37249 non-null  int64 \n","dtypes: int64(1), object(1)\n","memory usage: 582.1+ KB\n","None\n"]}]},{"cell_type":"code","source":["# Handle missing values\n","twitter_df = twitter_df.dropna()\n","reddit_df = reddit_df.dropna()\n","\n","# Lowercase the text\n","twitter_df['cleaned_tweets'] = twitter_df['clean_text'].str.lower()\n","reddit_df['cleaned_comments'] = reddit_df['clean_comment'].str.lower()"],"metadata":{"id":"ynvxSWtI59B_","executionInfo":{"status":"ok","timestamp":1702312038501,"user_tz":300,"elapsed":6,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Tokenize Twitter text\n","tokenizer_twitter = Tokenizer()\n","tokenizer_twitter.fit_on_texts(twitter_df['cleaned_tweets'])\n","sequences_twitter = tokenizer_twitter.texts_to_sequences(twitter_df['cleaned_tweets'])\n","padded_sequences_twitter = pad_sequences(sequences_twitter, maxlen=250, padding='post')\n","\n","# Tokenize Reddit text\n","tokenizer_reddit = Tokenizer()\n","tokenizer_reddit.fit_on_texts(reddit_df['cleaned_comments'])\n","sequences_reddit = tokenizer_reddit.texts_to_sequences(reddit_df['cleaned_comments'])\n","padded_sequences_reddit = pad_sequences(sequences_reddit, maxlen=250, padding='post')\n"],"metadata":{"id":"GBpiiTkU5_jV","executionInfo":{"status":"ok","timestamp":1702312068952,"user_tz":300,"elapsed":30456,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=len(tokenizer_reddit.word_index) + 1, output_dim=100, input_length=padded_sequences_reddit.shape[1]))\n","model.add(LSTM(20))\n","model.add(Dense(3, activation='softmax'))\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'],run_eagerly=True)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xk68cqT4zTXh","executionInfo":{"status":"ok","timestamp":1702312069736,"user_tz":300,"elapsed":794,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}},"outputId":"1c6ba2ef-bf3c-4775-bce7-3436bdcf28e2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 250, 100)          5472000   \n","                                                                 \n"," lstm (LSTM)                 (None, 20)                9680      \n","                                                                 \n"," dense (Dense)               (None, 3)                 63        \n","                                                                 \n","=================================================================\n","Total params: 5481743 (20.91 MB)\n","Trainable params: 5481743 (20.91 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","model1 = Sequential()\n","model1.add(Embedding(input_dim=len(tokenizer_twitter.word_index) + 1, output_dim=100, input_length=padded_sequences_twitter.shape[1]))\n","model1.add(LSTM(20))\n","model1.add(Dense(3, activation='softmax'))\n","model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'],run_eagerly=True)\n","model1.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fvnc7tZSd3LW","executionInfo":{"status":"ok","timestamp":1702312070141,"user_tz":300,"elapsed":415,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}},"outputId":"7e096696-2c55-46ee-86f4-45296ee8bb91"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 250, 100)          11367900  \n","                                                                 \n"," lstm_1 (LSTM)               (None, 20)                9680      \n","                                                                 \n"," dense_1 (Dense)             (None, 3)                 63        \n","                                                                 \n","=================================================================\n","Total params: 11377643 (43.40 MB)\n","Trainable params: 11377643 (43.40 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.utils import to_categorical\n","# Assuming 'sentiment_label' is the column containing sentiment labels\n","\n","# Convert labels to categorical format\n","labels_reddit = to_categorical(reddit_df['category'],num_classes=3)\n","\n","# Train-test split for Reddit data\n","X_train_reddit, X_test_reddit, y_train_reddit, y_test_reddit = train_test_split(\n","    padded_sequences_reddit, labels_reddit, test_size=0.2, random_state=42\n",")\n","# Further split the training set into training and validation sets\n","X_train_reddit, X_val_reddit, y_train_reddit, y_val_reddit = train_test_split(\n","    X_train_reddit, y_train_reddit, test_size=0.2, random_state=42\n",")\n","import numpy as np\n","print(X_train_reddit.shape, y_train_reddit.shape)\n","print(X_val_reddit.shape, y_val_reddit.shape)\n","\n","print(np.any(np.isnan(X_train_reddit)))\n","print(np.any(np.isnan(y_train_reddit)))\n","\n","print(X_train_reddit[:5])\n","print(y_train_reddit[:5])\n","\n","# Train the model on Reddit data\n","model.fit(X_train_reddit, y_train_reddit, epochs=5, batch_size=10, validation_data=(X_val_reddit, y_val_reddit))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQUyPAPR1TK0","executionInfo":{"status":"ok","timestamp":1702323852672,"user_tz":300,"elapsed":11782535,"user":{"displayName":"Jaswanth Kumar Kandipilli","userId":"00885504620875276652"}},"outputId":"b2252410-0d70-4299-e46a-3e68cfac27d6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(23775, 250) (23775, 3)\n","(5944, 250) (5944, 3)\n","False\n","False\n","[[    6     7   603 ...     0     0     0]\n"," [ 1152   858 32909 ...     0     0     0]\n"," [   20    19     8 ...     0     0     0]\n"," [   75    70     2 ...     0     0     0]\n"," [ 1620  4317    17 ...     0     0     0]]\n","[[0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]]\n","Epoch 1/5\n","2378/2378 [==============================] - 2363s 994ms/step - loss: 1.0670 - accuracy: 0.4244 - val_loss: 1.0627 - val_accuracy: 0.4241\n","Epoch 2/5\n","2378/2378 [==============================] - 2295s 965ms/step - loss: 1.0089 - accuracy: 0.5092 - val_loss: 0.9757 - val_accuracy: 0.5553\n","Epoch 3/5\n","2378/2378 [==============================] - 2326s 978ms/step - loss: 0.9929 - accuracy: 0.5285 - val_loss: 0.9536 - val_accuracy: 0.5616\n","Epoch 4/5\n","2378/2378 [==============================] - 2398s 1s/step - loss: 0.8746 - accuracy: 0.6122 - val_loss: 0.7636 - val_accuracy: 0.6699\n","Epoch 5/5\n","2378/2378 [==============================] - 2350s 988ms/step - loss: 0.5911 - accuracy: 0.7543 - val_loss: 0.5751 - val_accuracy: 0.7537\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7f998d9ae1d0>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.utils import to_categorical\n","# Assuming 'sentiment_label' is the column containing sentiment labels\n","\n","# Convert labels to categorical format\n","labels_twitter = to_categorical(twitter_df['category'],num_classes=3)\n","\n","# Train-test split for Twitter data\n","X_train_twitter, X_test_twitter, y_train_twitter, y_test_twitter= train_test_split(\n","    padded_sequences_twitter, labels_twitter, test_size=0.2, random_state=42\n",")\n","# Further split the training set into training and validation sets\n","X_train_twitter, X_val_twitter, y_train_twitter, y_val_twitter = train_test_split(\n","    X_train_twitter, y_train_twitter, test_size=0.2, random_state=42\n",")\n","import numpy as np\n","print(X_train_twitter.shape, y_train_twitter.shape)\n","print(X_val_twitter.shape, y_val_twitter.shape)\n","\n","print(np.any(np.isnan(X_train_twitter)))\n","print(np.any(np.isnan(y_train_twitter)))\n","\n","print(X_train_twitter[:5])\n","print(y_train_twitter[:5])\n","\n","# Train the model on Twitter data\n","model1.fit(X_train_twitter, y_train_twitter, epochs=5, batch_size=13, validation_data=(X_val_twitter, y_val_twitter))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9T5VGgK0Zd1N","outputId":"3328d52a-f696-4d4b-f42b-1fc29591afab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(104300, 250) (104300, 3)\n","(26075, 250) (26075, 3)\n","False\n","False\n","[[64414    16    10 ...     0     0     0]\n"," [  201   107  1934 ...     0     0     0]\n"," [  126  1931  1272 ...     0     0     0]\n"," [14848  8967  1334 ...     0     0     0]\n"," [  186    35  1962 ...     0     0     0]]\n","[[0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n","Epoch 1/5\n","8024/8024 [==============================] - 10974s 1s/step - loss: 1.0599 - accuracy: 0.4430 - val_loss: 1.0589 - val_accuracy: 0.4472\n","Epoch 2/5\n","8024/8024 [==============================] - 11414s 1s/step - loss: 1.0594 - accuracy: 0.4431 - val_loss: 1.0590 - val_accuracy: 0.4472\n","Epoch 3/5\n","6795/8024 [========================>.....] - ETA: 26:05 - loss: 1.0595 - accuracy: 0.4425"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","\n","# Assuming 'model' is your trained model and 'embedding_layer' is the Embedding layer in your model\n","\n","# Extract the weights of the embedding layer\n","embedding_weights = model.get_layer('embedding_2').get_weights()[0]\n","\n","# Define a subset of the embedding weights (optional: you can use all weights)\n","subset_embedding_weights = embedding_weights[:1000]  # Adjust the subset size based on your preference\n","\n","# Apply t-SNE to reduce the dimensionality of the embeddings\n","tsne = TSNE(n_components=2, random_state=42)\n","embeddings_tsne = tsne.fit_transform(subset_embedding_weights)\n","\n","# Visualize the embeddings for all sentiments\n","plt.figure(figsize=(12, 10))\n","\n","# Plot all embeddings\n","plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], alpha=0.5, label='All')\n","\n","# Annotate a few points with words for better understanding\n","for i in range(len(subset_embedding_weights)):\n","    plt.annotate(i, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]))\n","\n","# Visualize the embeddings for positive sentiment (adjust indices accordingly)\n","# positive_indices = [i for i in range(len(subset_embedding_weights)) if y_test_reddit[i, 1] == 1]\n","# plt.scatter(embeddings_tsne[positive_indices, 0], embeddings_tsne[positive_indices, 1], alpha=0.5, label='Positive', color='green')\n","\n","# Visualize the embeddings for negative sentiment (adjust indices accordingly)\n","# negative_indices = [i for i in range(len(subset_embedding_weights)) if y_test_reddit[i, 0] == 1]\n","# plt.scatter(embeddings_tsne[negative_indices, 0], embeddings_tsne[negative_indices, 1], alpha=0.5, label='Negative', color='red')\n","\n","plt.title('t-SNE Visualization of Word Embeddings')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"rtcJR3-UVgZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","\n","# Assuming 'model' is your trained model and 'embedding_layer' is the Embedding layer in your model\n","\n","# Extract the weights of the embedding layer\n","embedding_weights = model.get_layer('embedding_2').get_weights()[0]\n","\n","# Define a subset of the embedding weights (optional: you can use all weights)\n","subset_embedding_weights = embedding_weights[:1000]  # Adjust the subset size based on your preference\n","\n","# Apply t-SNE to reduce the dimensionality of the embeddings\n","tsne = TSNE(n_components=2, random_state=42)\n","embeddings_tsne = tsne.fit_transform(subset_embedding_weights)\n","\n","# Visualize the embeddings for all sentiments\n","plt.figure(figsize=(12, 10))\n","\n","# Plot all embeddings\n","# plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], alpha=0.5, label='All')\n","\n","# Annotate a few points with words for better understanding\n","for i in range(len(subset_embedding_weights)):\n","    plt.annotate(i, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]))\n","\n","# Visualize the embeddings for positive sentiment (adjust indices accordingly)\n","positive_indices = [i for i in range(len(subset_embedding_weights)) if y_test_reddit[i, 1] == 1]\n","plt.scatter(embeddings_tsne[positive_indices, 0], embeddings_tsne[positive_indices, 1], alpha=0.5, label='Positive', color='green')\n","\n","# Visualize the embeddings for negative sentiment (adjust indices accordingly)\n","# negative_indices = [i for i in range(len(subset_embedding_weights)) if y_test_reddit[i, 0] == 1]\n","# plt.scatter(embeddings_tsne[negative_indices, 0], embeddings_tsne[negative_indices, 1], alpha=0.5, label='Negative', color='red')\n","\n","plt.title('t-SNE Visualization of Word Embeddings')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"IY_lUktLZB2Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","\n","# Assuming 'model' is your trained model and 'embedding_layer' is the Embedding layer in your model\n","\n","# Extract the weights of the embedding layer\n","embedding_weights = model.get_layer('embedding_2').get_weights()[0]\n","\n","# Define a subset of the embedding weights (optional: you can use all weights)\n","subset_embedding_weights = embedding_weights[:1000]  # Adjust the subset size based on your preference\n","\n","# Apply t-SNE to reduce the dimensionality of the embeddings\n","tsne = TSNE(n_components=2, random_state=42)\n","embeddings_tsne = tsne.fit_transform(subset_embedding_weights)\n","\n","# Visualize the embeddings for all sentiments\n","plt.figure(figsize=(12, 10))\n","\n","# Plot all embeddings\n","# plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], alpha=0.5, label='All')\n","\n","# Annotate a few points with words for better understanding\n","for i in range(len(subset_embedding_weights)):\n","    plt.annotate(i, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]))\n","\n","# Visualize the embeddings for positive sentiment (adjust indices accordingly)\n","# positive_indices = [i for i in range(len(subset_embedding_weights)) if y_test_reddit[i, 1] == 1]\n","# plt.scatter(embeddings_tsne[positive_indices, 0], embeddings_tsne[positive_indices, 1], alpha=0.5, label='Positive', color='green')\n","\n","# Visualize the embeddings for negative sentiment (adjust indices accordingly)\n","negative_indices = [i for i in range(len(subset_embedding_weights)) if y_test_reddit[i, 0] == 1]\n","plt.scatter(embeddings_tsne[negative_indices, 0], embeddings_tsne[negative_indices, 1], alpha=0.5, label='Negative', color='red')\n","\n","plt.title('t-SNE Visualization of Word Embeddings')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"D0iiJro1ZGns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","\n","# Assuming 'model1' is your trained model and 'embedding_layer' is the Embedding layer in your model\n","\n","# Extract the weights of the embedding layer\n","embedding_weights = model1.get_layer('embedding_2').get_weights()[0]\n","\n","# Define a subset of the embedding weights (optional: you can use all weights)\n","subset_embedding_weights = embedding_weights[:1000]  # Adjust the subset size based on your preference\n","\n","# Apply t-SNE to reduce the dimensionality of the embeddings\n","tsne = TSNE(n_components=2, random_state=42)\n","embeddings_tsne = tsne.fit_transform(subset_embedding_weights)\n","\n","# Visualize the embeddings for all sentiments\n","plt.figure(figsize=(12, 10))\n","\n","# Plot all embeddings\n","plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], alpha=0.5, label='All')\n","\n","# Annotate a few points with words for better understanding\n","for i in range(len(subset_embedding_weights)):\n","    plt.annotate(i, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]))\n","\n","# Visualize the embeddings for positive sentiment (adjust indices accordingly)\n","positive_indices = [i for i in range(len(subset_embedding_weights)) if y_test_reddit[i, 1] == 1]\n","plt.scatter(embeddings_tsne[positive_indices, 0], embeddings_tsne[positive_indices, 1], alpha=0.5, label='Positive', color='green')\n","\n","# Visualize the embeddings for negative sentiment (adjust indices accordingly)\n","negative_indices = [i for i in range(len(subset_embedding_weights)) if y_test_reddit[i, 0] == 1]\n","plt.scatter(embeddings_tsne[negative_indices, 0], embeddings_tsne[negative_indices, 1], alpha=0.5, label='Negative', color='red')\n","\n","plt.title('t-SNE Visualization of Word Embeddings')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"IyWxobXIVzDG"},"execution_count":null,"outputs":[]}]}